{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kpcb\n",
      "8835\n",
      "fabp4\n",
      "2797\n"
     ]
    }
   ],
   "source": [
    "# from utils import *\n",
    "from model import *\n",
    "from dataPre import *\n",
    "\n",
    "# from visualization.attention_visualization import createHTML\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import os,sys\n",
    "import json\n",
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# import re\n",
    "import math\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# embeddings = None\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.set_device(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(attention_model,train_loader,criterion,optimizer,epochs = 5,use_regularization = False,C=0,clip=False):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            attention_model : {object} model\n",
    "            train_loader    : {DataLoader} training data loaded into a dataloader\n",
    "            optimizer       :  optimizer\n",
    "            criterion       :  loss function. Must be BCELoss for binary_classification and NLLLoss for multiclass\n",
    "            epochs          : {int} number of epochs\n",
    "            use_regularizer : {bool} use penalization or not\n",
    "            C               : {int} penalization coeff\n",
    "            clip            : {bool} use gradient clipping or not\n",
    "       \n",
    "        Returns:\n",
    "            accuracy and losses of the model\n",
    "        \"\"\"\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for i in range(epochs):\n",
    "        print(\"Running EPOCH\",i+1)\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        correct = 0\n",
    "        for batch_idx,(lines, contactmap,properties) in enumerate(train_loader):  \n",
    "            input, seq_lengths, y = make_variables(lines, properties,smiles_letters)\n",
    "            attention_model.hidden_state = attention_model.init_hidden()\n",
    "            contactmap = create_variable(contactmap)\n",
    "            y_pred,att = attention_model(input,contactmap)\n",
    "            #penalization AAT - I\n",
    "            if use_regularization:\n",
    "                attT = att.transpose(1,2)\n",
    "                identity = torch.eye(att.size(1))\n",
    "                identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,att.size(1),att.size(1))).cuda()\n",
    "                penal = attention_model.l2_matrix_norm(att@attT - identity)\n",
    "            if not bool(attention_model.type) :\n",
    "                #binary classification\n",
    "                #Adding a very small value to prevent BCELoss from outputting NaN's\n",
    "                correct+=torch.eq(torch.round(y_pred.type(torch.DoubleTensor).squeeze(1)),y.type(torch.DoubleTensor)).data.sum()\n",
    "                if use_regularization:\n",
    "                    loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1),y.type(torch.DoubleTensor))+(C * penal.cpu()/train_loader.batch_size)\n",
    "                else:\n",
    "                    loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1),y.type(torch.DoubleTensor))\n",
    "            total_loss+=loss.data\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() #retain_graph=True\n",
    "           \n",
    "            #gradient clipping\n",
    "            if clip:\n",
    "                torch.nn.utils.clip_grad_norm(attention_model.parameters(),0.5)\n",
    "            optimizer.step()\n",
    "            n_batches+=1\n",
    "            if batch_idx %20000==0: \n",
    "                print(batch_idx)\n",
    "                \n",
    "        avg_loss = total_loss/n_batches\n",
    "        acc = correct.numpy()/(len(train_loader.dataset))\n",
    "        \n",
    "        losses.append(avg_loss)\n",
    "        accs.append(acc)\n",
    "        \n",
    "        print(\"avg_loss is\",avg_loss)\n",
    "        print(\"train ACC = \",acc)\n",
    "        \n",
    "        torch.save(attention_model.state_dict(), './model_pkl/DUDE/hhDUDE30Res-fold1-%d.pkl'%(i+1))\n",
    "    return losses,accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getROCE(predList,targetList,roceRate):\n",
    "    p = sum(targetList)\n",
    "    n = len(targetList) - p\n",
    "    predList = [[index,x] for x in enumerate(predList)]\n",
    "    predList = sorted(predList,key = lambda x:x[1])\n",
    "    tp1 = 0\n",
    "    fp1 = 0\n",
    "    maxIndexs = []\n",
    "    for x in predList:\n",
    "        if(targetList[x[0]] == 1):\n",
    "            tp1 += 1\n",
    "        else:\n",
    "            fp1 += 1\n",
    "            if(fp1>((roceRate*n)/100)):\n",
    "                break\n",
    "    roce = (tp1*n)/(p*fp1)\n",
    "    return roce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(attention_model,test_loader,criterion,use_regularization = False):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            attention_model : {object} model\n",
    "            train_loader    : {DataLoader} training data loaded into a dataloader\n",
    "            optimizer       :  optimizer\n",
    "            criterion       :  loss function. Must be BCELoss for binary_classification and NLLLoss for multiclass\n",
    "            epochs          : {int} number of epochs\n",
    "            use_regularizer : {bool} use penalization or not\n",
    "            C               : {int} penalization coeff\n",
    "            clip            : {bool} use gradient clipping or not\n",
    "        Returns:\n",
    "            accuracy and losses of the model\n",
    "        \"\"\"\n",
    "    losses = []\n",
    "    accuracy = []\n",
    "    print('test begin ...')\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    correct = 0\n",
    "    all_pred = np.array([])\n",
    "    all_target = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for batch_idx,(lines, contactmap,properties) in enumerate(test_loader):\n",
    "            input, seq_lengths, y = make_variables(lines, properties,smiles_letters)\n",
    "            attention_model.hidden_state = attention_model.init_hidden()\n",
    "            contactmap = contactmap.cuda()\n",
    "            y_pred,att = attention_model(input,contactmap)\n",
    "            if not bool(attention_model.type) :\n",
    "                #binary classification\n",
    "                #Adding a very small value to prevent BCELoss from outputting NaN's\n",
    "                pred = torch.round(y_pred.type(torch.DoubleTensor).squeeze(1))\n",
    "                correct+=torch.eq(torch.round(y_pred.type(torch.DoubleTensor).squeeze(1)),y.type(torch.DoubleTensor)).data.sum()\n",
    "                all_pred=np.concatenate((all_pred,y_pred.data.cpu().squeeze(1).numpy()),axis = 0)\n",
    "                all_target = np.concatenate((all_target,y.data.cpu().numpy()),axis = 0)\n",
    "                if use_regularization:\n",
    "                    loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1),y.type(torch.DoubleTensor))+(C * penal.cpu()/train_loader.batch_size)\n",
    "                else:\n",
    "                    loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1),y.type(torch.DoubleTensor))\n",
    "            total_loss+=loss.data\n",
    "            n_batches+=1\n",
    "    testSize = round(len(test_loader.dataset),3)\n",
    "    testAcc = round(correct.numpy()/(n_batches*test_loader.batch_size),3)\n",
    "    testRecall = round(metrics.recall_score(all_target,np.round(all_pred)),3)\n",
    "    testPrecision = round(metrics.precision_score(all_target,np.round(all_pred)),3)\n",
    "    testAuc = round(metrics.roc_auc_score(all_target, all_pred),3)\n",
    "    testLoss = round(total_loss/n_batches,5)\n",
    "    print(\"test size =\",testSize,\"  test acc =\",testAcc,\"  test recall =\",testRecall,\"  test precision =\",testPrecision,\"  test auc =\",testAuc,\"  test loss = \",testLoss)\n",
    "    roce1 = round(getROCE(all_pred,all_target,0.5),2)\n",
    "    roce2 = round(getROCE(all_pred,all_target,1),2)\n",
    "    roce3 = round(getROCE(all_pred,all_target,2),2)\n",
    "    roce4 = round(getROCE(all_pred,all_target,5),2)\n",
    "    print(\"roce0.5 =\",roce1,\"  roce1.0 =\",roce2,\"  roce2.0 =\",roce3,\"  roce5.0 =\",roce4)\n",
    "    return testAcc,testRecall,testPrecision,testAuc,testLoss,all_pred,all_target,roce1,roce2,roce3,roce4\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classfication(attention_model,train_loader,epochs=20,use_regularization=True,C=1.0,clip=True):\n",
    "    attention_model.cuda()\n",
    "    loss = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(attention_model.parameters(),lr=0.0007)\n",
    "    losses,accs = train(attention_model,train_loader,loss,optimizer,epochs,use_regularization,C,clip)\n",
    "    return losses,accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running EPOCH 1\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-079961ff8a20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m attention_model = DrugVQA(batch_size=BATCH_SIZE,lstm_hid_dim=LSTM_HID_DIM,d_a = D_A,r=R_HEAD,block = ResidualBlock,\n\u001b[1;32m      8\u001b[0m                           n_chars_smi = N_CHARS_SMI,n_chars_seq = N_CHARS_SEQ,num_classes=[2, 2, 2, 2],n_classes=1)\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_classfication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_regularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-3e87c6a1e069>\u001b[0m in \u001b[0;36mbinary_classfication\u001b[0;34m(attention_model, train_loader, epochs, use_regularization, C, clip)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0007\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_regularization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3d6ddd28aac5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(attention_model, train_loader, criterion, optimizer, epochs, use_regularization, C, clip)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m#gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mn_batches\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     49\u001b[0m     warnings.warn(\"torch.nn.utils.clip_grad_norm is now deprecated in favor \"\n\u001b[1;32m     50\u001b[0m                   \"of torch.nn.utils.clip_grad_norm_.\", stacklevel=2)\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;34mr\"\"\"See :func: `torch.norm`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbtrifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LSTM_HID_DIM = 64\n",
    "D_A = 32\n",
    "R_HEAD = 10\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 50\n",
    "\n",
    "attention_model = DrugVQA(batch_size=BATCH_SIZE,lstm_hid_dim=LSTM_HID_DIM,d_a = D_A,r=R_HEAD,block = ResidualBlock,\n",
    "                          n_chars_smi = N_CHARS_SMI,n_chars_seq = N_CHARS_SEQ,num_classes=[2, 2, 2, 2],n_classes=1)\n",
    "losses,accs = binary_classfication(attention_model,train_loader=train_loader,epochs=EPOCHS,use_regularization=False,C=0.03,clip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "1\n",
      "-------------------------------testEpoch-------------------------------- DUDE30Res-fold1-15.pkl\n",
      "\n",
      " current test protein: kpcb_2i0eA_full\n",
      "test begin ...\n"
     ]
    }
   ],
   "source": [
    "LSTM_HID_DIM = 64\n",
    "D_A = 32\n",
    "R_HEAD = 10\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 50\n",
    "attention_model = DrugVQA(batch_size=BATCH_SIZE,lstm_hid_dim=LSTM_HID_DIM,d_a = D_A,r=R_HEAD,block = ResidualBlock,\n",
    "                          n_chars_smi = N_CHARS_SMI,n_chars_seq = N_CHARS_SEQ,num_classes=[2, 2, 2, 2],n_classes=1)\n",
    "attention_model.cuda()\n",
    "testpklPath =  './model_pkl/DUDE/'\n",
    "testpkl = os.listdir(testpklPath)\n",
    "print(len(testpkl))\n",
    "testpkl = [x for x in testpkl if 'DUDE30Res-fold1' in x and '15' in x]\n",
    "testpkl = sorted(testpkl)\n",
    "print(len(testpkl))\n",
    "resultDict = {}\n",
    "for path in testpkl:\n",
    "    resultProteinDict = {}\n",
    "    attention_model.load_state_dict(torch.load('./model_pkl/DUDE/'+path,map_location=lambda storage,loc:storage.cuda(4)))\n",
    "    loss = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(attention_model.parameters())\n",
    "    print('-------------------------------testEpoch--------------------------------',path)\n",
    "    for x in testProteinList:\n",
    "        print('\\n current test protein:',x)\n",
    "        data = dataDict[x]\n",
    "        test_dataset = ProDataset(dataSet = data,seqContactDict = seqContactDict)\n",
    "        test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=1, shuffle=True,drop_last = True)\n",
    "        \n",
    "        testAcc,testRecall,testPrecision,testAuc,testLoss,all_pred,all_target,roce1,roce2,roce3,roce4= test(attention_model,test_loader,loss)\n",
    "        resultProteinDict[x] = [testAcc,testRecall,testPrecision,testAuc,testLoss,roce1,roce2,roce3,roce4]\n",
    "    resultDict[path] = resultProteinDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
